# -*- coding: utf-8 -*-
"""01_LNCC_bruno_MC-CD05.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/124KFgfKZaLMBT9eP4gCT79_SDeEESrRD

##**Large Language Models - MC-CD05**
*Professor: Bruno Menezes e Daniel Senna (LNCC)*

#Núcleo do Transformer: atenção, embeddings e normalização.
"""

# ============================================================
# COLAB DEMO — NÚCLEO DO TRANSFORMER
# BERT (bidirecional)
#
# Objetivo:
# - Mostrar como o Transformer transforma palavras em vetores inteligentes:
#   * quem eu sou        -> token embeddings
#   * onde eu estou     -> position embeddings
#   * com quem me importo -> self-attention
# - Começar com BERT (atenção bidirecional, mais didática)
#
# ============================================================

# -----------------------------
# CÉLULA 1 — Instalar dependências
# -----------------------------
!pip -q install transformers accelerate

# -----------------------------
# CÉLULA 2 — Imports
# -----------------------------
import time                                # Medição de tempo
import torch                               # Tensores e execução em CPU/GPU
import numpy as np                         # Operações numéricas
import matplotlib.pyplot as plt            # Visualização das matrizes de atenção
from transformers import (
    AutoTokenizer,
    AutoModel,
    AutoModelForCausalLM
)

# -----------------------------
# CÉLULA 3 — Seleção de dispositivo
# -----------------------------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)

# ============================================================
# PARTE A — BERT (TRANSFORMER BIDIRECIONAL)
# ============================================================

# -----------------------------
# CÉLULA 4 — Carregar tokenizer e modelo BERT
# -----------------------------
bert_name = "bert-base-uncased"                            # Modelo encoder bidirecional
bert_tokenizer = AutoTokenizer.from_pretrained(bert_name)  # Tokenizer WordPiece
bert_model = AutoModel.from_pretrained(bert_name)          # BERT base (sem head)
bert_model.to(device)                                      # Move para GPU/CPU
bert_model.eval()                                          # Modo avaliação

# -----------------------------
# CÉLULA 5 — Texto de exemplo
# -----------------------------
text = "A cute teddy bear is reading a book."
print("\n[BERT] Input text:", text)

# -----------------------------
# CÉLULA 6 — Tokenização (BERT)
# -----------------------------
# Aplica o tokenizer do BERT ao texto bruto.
# Nesta etapa o texto é:
#  - normalizado (minúsculas, pois o modelo é uncased)
#  - segmentado em WordPieces
#  - enriquecido com tokens especiais [CLS] (início) e [SEP] (fim)
#  - convertido em tensores PyTorch
bert_inputs = bert_tokenizer(text, return_tensors="pt")

# Extrai os IDs numéricos dos tokens.
# Cada número corresponde a um token no vocabulário fixo do BERT.
# Shape: [batch_size, seq_len]
bert_input_ids = bert_inputs["input_ids"].to(device)

# Extrai a máscara de atenção.
# Valor 1 indica token válido; valor 0 indicaria padding.
# Essa máscara será usada para impedir que o modelo atenda posições inexistentes.
bert_attention_mask = bert_inputs["attention_mask"].to(device)

# Converte os IDs numéricos de volta para tokens legíveis.
# Isso NÃO é usado no modelo, apenas para inspeção didática.
# Permite visualizar:
#  - tokens especiais ([CLS], [SEP])
#  - possíveis quebras em subpalavras
bert_tokens = bert_tokenizer.convert_ids_to_tokens(
    bert_input_ids[0].tolist()
)

# Imprime os tokens que efetivamente entram no Transformer BERT.
# O índice de cada token corresponde às linhas/colunas das matrizes de atenção.
print("[BERT] Tokens:")

# Itera sobre os tokens, exibindo posição e símbolo textual.
# Essa numeração será usada depois para interpretar mapas de self-attention.
for i, t in enumerate(bert_tokens):
    print(f"{i:02d}  {t}")

# ============================================================
# CÉLULA 6.1 — Prints tokenização BERT
# ============================================================

print("\n==============================")
print("[PRINT 1] Texto de entrada")
print("==============================")
print(text)

print("\n==============================")
print("[PRINT 2] Tokens gerados pelo BERT (WordPiece)")
print("==============================")
for i, t in enumerate(bert_tokens):
    print(f"{i:02d}  {t}")

print("\n==============================")
print("[PRINT 3] input_ids (IDs numéricos dos tokens)")
print("==============================")
print("Shape:", tuple(bert_input_ids.shape))
print(bert_input_ids)

print("\n==============================")
print("[PRINT 4] attention_mask (tokens válidos)")
print("==============================")
print("Shape:", tuple(bert_attention_mask.shape))
print(bert_attention_mask)

print("\n==============================")
print("[INTERPRETAÇÃO GUIADA]")
print("==============================")
print(
    "• Cada linha em input_ids representa um token do texto.\n"
    "• Os números são índices no vocabulário do BERT.\n"
    "• attention_mask = 1 indica token válido.\n"
    "• A posição do token define sua linha/coluna nas matrizes de self-attention.\n"
    "• Esses objetos são a única entrada real do Transformer."
)

# -----------------------------
# CÉLULA 7 — Embeddings de entrada (quem + onde + segmento)
# -----------------------------
# Nesta célula, o texto deixa definitivamente de ser "palavra" e passa a ser
# uma sequência de vetores numéricos que o Transformer consegue processar.
#
# Todas as representações terão o formato:
# (batch_size, seq_len, hidden_size)
#
# No caso do BERT-base:
#  - batch_size = 1  (uma frase)
#  - seq_len    = número de tokens (inclui [CLS] e [SEP])
#  - hidden_size = 768 (dimensão do espaço vetorial do modelo)

with torch.no_grad():
    # Comprimento da sequência após a tokenização
    # Ex.: [CLS] a cute teddy bear is reading a book . [SEP] -> 11 tokens
    seq_len_bert = bert_input_ids.shape[1]

    # Cria os IDs de posição para cada token da sequência
    # Esses IDs informam ao modelo "onde eu estou" na frase
    # Shape: (1, seq_len)
    position_ids = torch.arange(seq_len_bert, device=device).unsqueeze(0)

    # Cria os IDs de segmento (token type IDs)
    # No BERT, eles servem para diferenciar frases A e B (ex.: pergunta/resposta)
    # Como aqui há apenas uma frase, todos os valores são zero
    # Shape: (1, seq_len)
    token_type_ids = torch.zeros_like(bert_input_ids)

    # Busca os embeddings dos tokens no vocabulário do BERT
    # Cada token ID é convertido em um vetor de 768 dimensões
    # Representa "quem eu sou" (significado lexical bruto)
    # Shape: (1, seq_len, 768)
    token_embeds = bert_model.embeddings.word_embeddings(bert_input_ids)

    # Busca os embeddings posicionais
    # Cada posição da sequência possui um vetor próprio
    # Representa "onde eu estou" na frase
    # Shape: (1, seq_len, 768)
    position_embeds = bert_model.embeddings.position_embeddings(position_ids)

    # Busca os embeddings de segmento
    # Indica a qual frase o token pertence (A ou B)
    # Aqui não adiciona informação semântica nova, mas faz parte da arquitetura
    # Shape: (1, seq_len, 768)
    segment_embeds = bert_model.embeddings.token_type_embeddings(token_type_ids)

    # Soma dos três tipos de embedding
    # Este tensor representa a entrada final do Transformer
    # Antes da aplicação de LayerNorm e Dropout
    #
    # Em termos conceituais:
    # token_embeds    -> "quem eu sou"
    # position_embeds -> "onde eu estou"
    # segment_embeds  -> "a qual frase pertenço"
    bert_input_embeds = token_embeds + position_embeds + segment_embeds

# Impressão das dimensões para inspeção didática
# Todas devem ter o mesmo formato (1, seq_len, 768)
print("\n[BERT] Shapes:")
print(" token_embeds    :", tuple(token_embeds.shape))
print(" position_embeds :", tuple(position_embeds.shape))
print(" segment_embeds  :", tuple(segment_embeds.shape))
print(" input_embeds    :", tuple(bert_input_embeds.shape))

print(" token_embeds    :", token_embeds)
print(" position_embeds :", position_embeds)
print(" segment_embeds  :", segment_embeds)
print(" input_embeds    :", bert_input_embeds)

# ------------------------------------------------------------
# INSPEÇÃO DOS EMBEDDINGS — LEITURA GUIADA DOS TENSORES
# ------------------------------------------------------------
# A partir daqui, estamos olhando diretamente para os vetores
# que representam os tokens no espaço interno do BERT.
#
# Cada tensor tem o formato:
# (batch_size = 1, seq_len = 11, hidden_size = 768)
#
# • batch_size = 1
#   Indica quantas sequências estão sendo processadas em paralelo.
#   Aqui temos apenas uma frase, mas em treino real esse valor pode ser 8, 16, 32, etc.
#   O Transformer sempre trabalha em batch, mesmo quando processa uma única frase.
#
# • seq_len = 11
#   Corresponde ao número total de tokens após a tokenização.
#   Inclui tokens especiais do BERT, como [CLS] (início) e [SEP] (fim).
#   Cada token ocupa uma posição fixa na sequência e terá um vetor próprio.
#
# • hidden_size = 768
#   É a dimensão do espaço vetorial interno do BERT-base.
#   Cada token é representado por um vetor de 768 números reais.
#   Esse valor é uma característica arquitetural do modelo (BERT-large usa 1024).
#
# Em resumo:
# Uma frase -> 11 tokens -> cada token vira um vetor 768D
# Esse tensor é a representação que alimenta todas as camadas do Transformer.
#
# Cada linha da dimensão seq_len corresponde a um token da frase.
# Cada vetor de 768 valores representa esse token em um espaço semântico.

# ------------------------------------------------------------
# token_embeds
# ------------------------------------------------------------
# Embedding lexical dos tokens ("quem eu sou").
#
# • Cada token ID busca um vetor fixo na tabela de embeddings do BERT.
# • Esses vetores NÃO dependem do contexto.
# • Ex.: o vetor de "teddy" aqui é o mesmo em qualquer frase.
# • Valores positivos e negativos não têm interpretação isolada:
#   o significado está no vetor como um todo (geometria).
print(" token_embeds    :", token_embeds)

# ------------------------------------------------------------
# position_embeds
# ------------------------------------------------------------
# Embedding posicional ("onde eu estou").
#
# • Cada posição da sequência (0, 1, 2, ...) possui um vetor próprio.
# • Dois tokens iguais em posições diferentes terão embeddings diferentes.
# • Esses vetores permitem ao Transformer distinguir ordem e distância.
# • Sem isso, a frase seria tratada como um conjunto sem ordem.
print(" position_embeds :", position_embeds)

# ------------------------------------------------------------
# segment_embeds
# ------------------------------------------------------------
# Embedding de segmento ("a qual frase eu pertenço").
#
# • Usado no BERT para diferenciar sentença A e sentença B
#   (ex.: pergunta vs resposta).
# • Como aqui há apenas uma frase, todos os vetores são iguais.
# • Apesar disso, o embedding ainda é somado por consistência arquitetural.
print(" segment_embeds  :", segment_embeds)

# ------------------------------------------------------------
# input_embeds
# ------------------------------------------------------------
# Embedding final de entrada do Transformer.
#
# • Resultado da soma:
#   token_embeds + position_embeds + segment_embeds
#
# • Este tensor é a representação completa do texto ANTES da atenção.
# • A partir daqui, o modelo NÃO enxerga mais palavras ou posições explícitas.
# • Tudo o que acontece nas camadas seguintes (Q, K, V, atenção, FFN)
#   opera exclusivamente sobre esses vetores.
print(" input_embeds    :", bert_input_embeds)

"""## Uma representação visual"""

# ============================================================
# VISUALIZAÇÕES DIDÁTICAS — GEOMETRIA DOS EMBEDDINGS E ATENÇÃO
# ============================================================
# Este código cria visualizações progressivas:
#
# 1) Espaço geométrico dos embeddings (PCA)
# 2) (Opcional) Heatmap de atenção
# 3) (Opcional) Setas de atenção sobre o espaço geométrico
#
# IMPORTANTE (didático):
# • Se bert_attentions NÃO existir ainda, apenas a geometria é exibida
# • Isso permite explicar primeiro "onde os tokens estão"
# • A atenção entra somente quando você decidir
# ============================================================

import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

# ------------------------------------------------------------
# PREPARAÇÃO DOS DADOS
# ------------------------------------------------------------
# bert_input_embeds: (1, seq_len, 768)
# Removemos batch → (seq_len, 768)

X = bert_input_embeds[0].detach().cpu().numpy()
seq_len = X.shape[0]

# ------------------------------------------------------------
# 1) PROJEÇÃO GEOMÉTRICA (PCA)
# ------------------------------------------------------------
# PCA NÃO faz parte do Transformer.
# Ele serve apenas para reduzir 768D → 2D
# para permitir visualização humana.

pca = PCA(n_components=2)
X_2d = pca.fit_transform(X)

plt.figure(figsize=(8, 6))

# Cada token vira um ponto
plt.scatter(X_2d[:, 0], X_2d[:, 1], s=60, zorder=2)

# Rótulos dos tokens
for i, token in enumerate(bert_tokens):
    plt.text(
        X_2d[i, 0] + 0.02,
        X_2d[i, 1] + 0.02,
        f"{i}:{token}",
        fontsize=9
    )

plt.axhline(0, color="gray", linewidth=0.5)
plt.axvline(0, color="gray", linewidth=0.5)

plt.title(
    "Espaço geométrico dos embeddings de entrada (PCA)\n"
    "ANTES da atenção — apenas identidade + posição + segmento"
)
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.grid(True)
plt.show()

# ------------------------------------------------------------
# MENSAGEM PEDAGÓGICA (para você verbalizar):
#
# • Cada ponto = um token
# • Distância = relação geométrica
# • NÃO existe inteligência ainda
# • Não há contexto, só posição no espaço
# ------------------------------------------------------------


# ============================================================
# A PARTIR DAQUI: ATENÇÃO (SE JÁ ESTIVER DISPONÍVEL)
# ============================================================

if "bert_attentions" in globals():

    # --------------------------------------------------------
    # 2) HEATMAP DE SELF-ATTENTION
    # --------------------------------------------------------

    layer = len(bert_attentions) - 1   # última camada
    head = 0                           # primeira cabeça

    # (seq_len, seq_len)
    att = bert_attentions[layer][0, head].detach().cpu().numpy()

    plt.figure(figsize=(8, 7))
    plt.imshow(att, aspect="auto", cmap="viridis")
    plt.colorbar(label="Peso de atenção")

    plt.xticks(range(seq_len), bert_tokens, rotation=90)
    plt.yticks(range(seq_len), bert_tokens)

    plt.xlabel("Keys (tokens observados)")
    plt.ylabel("Queries (token que pergunta)")

    plt.title(
        f"Heatmap de Self-Attention\n"
        f"Camada {layer}, Cabeça {head}\n"
        f"(cada linha soma 1)"
    )

    plt.tight_layout()
    plt.show()

    # --------------------------------------------------------
    # INTERPRETAÇÃO PEDAGÓGICA:
    #
    # • Cada linha responde: "Para quem EU olho?"
    # • Cada coluna responde: "Quem está me observando?"
    # • Atenção é decisão LOCAL de cada token
    # --------------------------------------------------------


    # --------------------------------------------------------
    # 3) SETAS DE ATENÇÃO SOBRE O ESPAÇO GEOMÉTRICO
    # --------------------------------------------------------

    top_k = 2
    min_weight = 0.12

    plt.figure(figsize=(8, 6))

    # Pontos
    plt.scatter(X_2d[:, 0], X_2d[:, 1], s=60, zorder=2)

    # Rótulos
    for i, token in enumerate(bert_tokens):
        plt.text(
            X_2d[i, 0] + 0.02,
            X_2d[i, 1] + 0.02,
            f"{i}:{token}",
            fontsize=9
        )

    # Setas de atenção
    for i in range(seq_len):
        idx = np.argsort(-att[i])[:top_k]

        for j in idx:
            w = att[i, j]

            if w >= min_weight and i != j:
                plt.arrow(
                    X_2d[i, 0],
                    X_2d[i, 1],
                    X_2d[j, 0] - X_2d[i, 0],
                    X_2d[j, 1] - X_2d[i, 1],
                    alpha=w,
                    width=0.003,
                    head_width=0.05,
                    length_includes_head=True,
                    color="black",
                    zorder=1
                )

    plt.axhline(0, color="gray", linewidth=0.5)
    plt.axvline(0, color="gray", linewidth=0.5)

    plt.title(
        f"Fluxo de Atenção sobre o Espaço Geométrico\n"
        f"Camada {layer}, Cabeça {head}\n"
        f"(setas = decisões de informação)"
    )

    plt.xlabel("PC1")
    plt.ylabel("PC2")
    plt.grid(True)
    plt.show()

    # --------------------------------------------------------
    # • Pontos NÃO são inteligentes
    # • Setas criam dependência contextual
    # • Vetores finais = soma ponderada dos vetores observados
    #
    # A inteligência está nas conexões.
    # --------------------------------------------------------

else:
    print(
        "bert_attentions ainda não foi gerado.\n"
        "Mostramos apenas a geometria dos embeddings.\n"
        "Quando a atenção for introduzida, este mesmo código\n"
        "passará automaticamente a exibir heatmap e setas."
    )

# -----------------------------
# CÉLULA 8 — Forward do BERT com attentions
# -----------------------------
# Objetivo desta célula:
# 1) Rodar o BERT "de verdade" (forward pass) com a frase já tokenizada
# 2) Pedir que ele devolva, além dos vetores finais, as matrizes de atenção internas
# 3) Guardar essas matrizes para visualizar e inspecionar

# torch.no_grad() = modo "somente inferência"
# (não calcula gradientes e não prepara nada para treino)
# Isso deixa mais rápido e economiza memória.
with torch.no_grad():

    # bert_model(...) executa todas as camadas do BERT.
    # Nós passamos:
    # - input_ids: os IDs numéricos dos tokens (a entrada real do modelo)
    # - attention_mask: 1 para tokens válidos e 0 para padding (aqui quase sempre tudo 1)
    # - output_attentions=True: pede para o BERT retornar também as atenções internas
    # - return_dict=True: retorna um objeto "organizado" (com campos nomeados)
    bert_outputs = bert_model(
        input_ids=bert_input_ids,
        attention_mask=bert_attention_mask,
        output_attentions=True,   # IMPORTANTÍSSIMO: sem isso, não recebemos as atenções
        return_dict=True
    )

# bert_outputs é um "pacote" com vários resultados.
# Um dos campos é .attentions
# Ele contém uma lista, uma por camada, com as matrizes de atenção.
bert_attentions = bert_outputs.attentions

# Quantas camadas o BERT-base tem? Normalmente 12.
print("\n[BERT] Número de camadas:", len(bert_attentions))

# Shape da atenção em UMA camada:
# Geralmente: [batch, heads, seq_len, seq_len]
# - batch: quantas frases (aqui 1)
# - heads: quantas cabeças de atenção (ex.: 12)
# - seq_len: número de tokens (inclui [CLS] e [SEP])
# - seq_len: novamente, porque cada token pode "olhar" para todos os tokens
print("[BERT] Shape da atenção (camada 0):", tuple(bert_attentions[0].shape))
# Exemplo esperado: (1, 12, 11, 11)

# -----------------------------
# CÉLULA 9 — Visualização da atenção (BERT)
# -----------------------------
# Como existem muitas atenções (várias camadas e várias cabeças),
# escolhemos UMA camada e UMA cabeça para visualizar primeiro.

bert_layer = 0   # camada 0 = primeira camada do BERT
bert_head = 0    # cabeça 0 = primeira cabeça

# Agora extraímos UMA matriz 2D (seq_len x seq_len)
# bert_attentions[bert_layer]      -> todas as atenções da camada escolhida
# [0, bert_head]                   -> batch 0 (primeira frase) e a cabeça escolhida
# .cpu().numpy()                   -> traz para CPU e converte para numpy (para plotar)
att = bert_attentions[bert_layer][0, bert_head].cpu().numpy()

# Plot do heatmap:
# - Eixo X: tokens "observados" (keys)
# - Eixo Y: token que está "olhando" (query)
# Cada linha mostra "para quem esse token dá atenção".
plt.figure(figsize=(8, 6))
plt.imshow(att, aspect="auto")
plt.title("[BERT] Self-Attention (bidirecional)")
plt.xlabel("Keys (tokens atendidos)")
plt.ylabel("Queries (token que atende)")
plt.xticks(range(len(bert_tokens)), bert_tokens, rotation=90)
plt.yticks(range(len(bert_tokens)), bert_tokens)
plt.colorbar()          # barra de cores = intensidade do peso de atenção
plt.tight_layout()
plt.show()

# -----------------------------
# CÉLULA 10 — Top-k atenções de um token (BERT)
# -----------------------------
# Além de ver o mapa inteiro, vamos escolher UM token e listar
# para quais tokens ele mais "olha" (maiores pesos na linha dele).

target_pos = 3  # posição do token escolhido na sequência (ex.: 3 pode ser "teddy")
k = 5           # quantos tokens mais atendidos queremos listar

# row = a linha da matriz de atenção correspondente ao token target_pos
# Essa linha é uma lista de pesos: um peso para cada token da frase.
row = att[target_pos]

# argsort(-row) ordena os índices do maior peso para o menor.
# Pegamos só os k primeiros índices.
top_idx = np.argsort(-row)[:k]

# Imprime o token alvo e os tokens mais atendidos por ele
print(f"\n[BERT] Token '{bert_tokens[target_pos]}' atende mais para:")
for j in top_idx:
    print(f"  -> pos {j:02d} '{bert_tokens[j]}'  peso={row[j]:.4f}")

"""# Variações modernas: Mixture of Experts (MoE), Rotary Embeddings e Attention Scaling (visão sintética e aplicada)."""

# ============================================================
# VARIAÇÕES MODERNAS DO TRANSFORMER
# Mixture of Experts (MoE), Rotary Embeddings e Attention Scaling
#
# Continuação direta do notebook:
# "Núcleo do Transformer: atenção, embeddings e normalização"
# ============================================================

# -----------------------------
# CÉLULA 1 — Dependências
# -----------------------------
!pip -q install torch numpy matplotlib

# -----------------------------
# CÉLULA 2 — Imports
# -----------------------------
import math
import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)

# ============================================================
# CÉLULA 0 — CONTINUIDADE DIDÁTICA
# Reuso explícito do exemplo do núcleo
# ============================================================

# Tokens simbólicos (mesmo espírito do mini-corpus anterior)
tokens = ["teddy", "bear", "reads"]

# Embeddings toy (d_model = 4, dois pares para RoPE)
X = torch.tensor([
    [ 1.0,  0.0,  1.0,  0.0],   # teddy
    [ 1.0,  1.0,  0.0,  1.0],   # bear
    [ 0.0,  1.0, -1.0,  1.0],   # reads
], device=device)

print("[CONTINUIDADE] Tokens:", tokens)
print("[CONTINUIDADE] Embeddings base X:\n", X)

# Para foco conceitual:
Q = X.clone()
K = X.clone()
V = X.clone()

positions = torch.arange(len(tokens), device=device)

# ============================================================
# PARTE 1 — ROTARY POSITIONAL EMBEDDINGS (RoPE)
# ============================================================

# -----------------------------
# CÉLULA 3 — Rotação 2D explícita
# -----------------------------
def rot2d(x, theta):
    c = math.cos(theta)
    s = math.sin(theta)
    x1, x2 = x[..., 0], x[..., 1]
    y1 = x1 * c - x2 * s
    y2 = x1 * s + x2 * c
    return torch.stack([y1, y2], dim=-1)

# Demonstração geométrica simples
theta = math.pi / 6  # 30 graus
demo = torch.tensor([[1.0, 0.0], [1.0, 1.0]], device=device)

print("\n[RoPE 2D] x:\n", demo)
print("[RoPE 2D] theta:", theta)
print("[RoPE 2D] rot(x):\n", rot2d(demo, theta))

# -----------------------------
# CÉLULA 4 — RoPE generalizado (dimensão par)
# -----------------------------
def rope_apply(q_or_k, positions, base=10000.0):
    d = q_or_k.shape[-1]
    assert d % 2 == 0, "Dimensão deve ser par"

    i = torch.arange(0, d, 2, device=q_or_k.device).float()
    inv_freq = 1.0 / (base ** (i / d))

    angles = positions[:, None].float() * inv_freq[None, :]
    cos = torch.cos(angles)
    sin = torch.sin(angles)

    x1 = q_or_k[:, 0::2]
    x2 = q_or_k[:, 1::2]

    y1 = x1 * cos - x2 * sin
    y2 = x1 * sin + x2 * cos

    out = torch.empty_like(q_or_k)
    out[:, 0::2] = y1
    out[:, 1::2] = y2
    return out

Qr = rope_apply(Q, positions)
Kr = rope_apply(K, positions)

print("\n[RoPE] Q original:\n", Q)
print("[RoPE] Q rotacionado:\n", Qr)

# -----------------------------
# CÉLULA 5 — Logits antes e depois do RoPE
# -----------------------------
def attn_logits(Q, K):
    d = Q.shape[-1]
    return (Q @ K.T) / math.sqrt(d)

logits_plain = attn_logits(Q, K)
logits_rope  = attn_logits(Qr, Kr)

print("\n[ATENÇÃO] Logits sem RoPE:\n", logits_plain)
print("[ATENÇÃO] Logits com RoPE:\n", logits_rope)

plt.figure(figsize=(10, 4))
plt.subplot(1, 2, 1)
plt.imshow(logits_plain.cpu(), aspect="auto")
plt.title("QKᵀ / √d — sem RoPE")
plt.colorbar()

plt.subplot(1, 2, 2)
plt.imshow(logits_rope.cpu(), aspect="auto")
plt.title("QKᵀ / √d — com RoPE")
plt.colorbar()

plt.tight_layout()
plt.show()

# ============================================================
# PARTE 2 — ATTENTION SCALING E TEMPERATURA
# ============================================================

# -----------------------------
# CÉLULA 6 — Softmax estável
# -----------------------------
def softmax(x):
    x = x - x.max(dim=-1, keepdim=True).values
    ex = torch.exp(x)
    return ex / ex.sum(dim=-1, keepdim=True)

# Usamos uma linha REAL de atenção (token "teddy")
row = logits_plain[0]

print("\n[BASE] Logits reais:", row.tolist())

# -----------------------------
# CÉLULA 7 — Efeito da escala
# -----------------------------
scales = [0.5, 1.0, 2.0, 4.0]
for s in scales:
    p = softmax(s * row)
    ent = -(p * torch.log(p + 1e-12)).sum().item()
    print(f"[scale={s}] softmax={p.cpu().numpy()}  entropy={ent:.4f}")

# -----------------------------
# CÉLULA 8 — Temperatura explícita
# -----------------------------
taus = [2.0, 1.0, 0.5, 0.25]

plt.figure(figsize=(7, 4))
for tau in taus:
    p = softmax(row / tau)
    ent = -(p * torch.log(p + 1e-12)).sum().item()
    print(f"[tau={tau}] softmax={p.cpu().numpy()}  entropy={ent:.4f}")
    plt.plot(p.cpu(), marker="o", label=f"tau={tau}")

plt.title("Temperatura controla a concentração da atenção")
plt.xlabel("token observado")
plt.ylabel("probabilidade")
plt.grid(True)
plt.legend()
plt.show()

# -----------------------------
# CÉLULA 9 — Por que dividir por √d
# -----------------------------
torch.manual_seed(0)

def random_qk_logits(n=8, dk=64):
    Q = torch.randn(n, dk, device=device)
    K = torch.randn(n, dk, device=device)
    return Q @ K.T

print("\n[ESCALA vs DIMENSÃO]")
for dk in [16, 64, 256, 1024]:
    L = random_qk_logits(dk=dk)
    print(
        f"dk={dk:4d}  "
        f"std(raw)={L.std():.3f}  "
        f"std(scaled)={(L / math.sqrt(dk)).std():.3f}"
    )

# ============================================================
# PARTE 3 — MIXTURE OF EXPERTS (MoE)
# ============================================================

# -----------------------------
# CÉLULA 10 — FFN denso (baseline)
# -----------------------------
class DenseFFN(nn.Module):
    def __init__(self, d_model=4, d_ff=16):
        super().__init__()
        self.w1 = nn.Linear(d_model, d_ff)
        self.w2 = nn.Linear(d_ff, d_model)
        self.act = nn.GELU()

    def forward(self, x):
        return self.w2(self.act(self.w1(x)))

H = X.clone()  # saída da atenção (didática)
ffn = DenseFFN().to(device)
Y_dense = ffn(H)

print("\n[FFN DENSO] saída:\n", Y_dense)

# -----------------------------
# CÉLULA 11 — MoE (toy) como substituto do FFN
# -----------------------------
class ToyMoE(nn.Module):
    def __init__(self, d_model=4, d_ff=16, n_experts=4, k=1):
        super().__init__()
        self.k = k
        self.gate = nn.Linear(d_model, n_experts)
        self.experts = nn.ModuleList(
            [DenseFFN(d_model, d_ff) for _ in range(n_experts)]
        )

    def forward(self, x):
        gate_logits = self.gate(x)
        gate_probs = torch.softmax(gate_logits, dim=-1)

        topk_vals, topk_idx = torch.topk(gate_probs, self.k, dim=-1)
        y = torch.zeros_like(x)

        for t in range(x.shape[0]):
            for r in range(self.k):
                e = topk_idx[t, r].item()
                w = topk_vals[t, r]
                y[t] += w * self.experts[e](x[t:t+1]).squeeze(0)

        load = torch.bincount(topk_idx[:, 0], minlength=len(self.experts)).float()
        return y, gate_probs, topk_idx, load

moe = ToyMoE().to(device)
Y_moe, probs, idx, load = moe(H)

print("\n[MoE] saída:\n", Y_moe)
print("[MoE] expert por token:", idx[:, 0].tolist())
print("[MoE] load por expert:", load.tolist())

# -----------------------------
# CÉLULA 12 — Visualização da carga
# -----------------------------
plt.figure(figsize=(6, 4))
plt.bar(range(len(load)), load.cpu())
plt.title("MoE Routing — tokens por expert")
plt.xlabel("expert")
plt.ylabel("tokens")
plt.grid(True, axis="y")
plt.show()

# -----------------------------
# CÉLULA 12 — Visualização da carga
# -----------------------------
plt.figure(figsize=(6, 4))
plt.bar(range(len(load)), load.cpu())
plt.title("MoE Routing — tokens por expert")
plt.xlabel("expert")
plt.ylabel("tokens")
plt.grid(True, axis="y")
plt.show()

"""#Controle e inferência: parâmetros de geração (temperatura, top-p) e gerenciamento de contexto (KV cache, expected-attention, cache merging)."""

# ============================================================
# DIAGRAMAS DIDÁTICOS: TRANSFORMER → BERT → GPT-2
# UX focada em comparação visual
# ============================================================

!pip install graphviz

from graphviz import Digraph
from IPython.display import display, Markdown

display(Markdown("# Comparação Visual: Transformer vs BERT vs GPT-2"))

# ============================================================
# 1. TRANSFORMER GENÉRICO
# ============================================================
t = Digraph("Transformer", format="png")
t.attr(rankdir="LR", size="10,4")
t.attr("node", shape="rect", style="rounded,filled")

t.node("t_in", "Input Tokens")
t.node("t_emb", "Token + Positional Embedding", fillcolor="#BBDEFB")
t.node("t_attn", "Self-Attention", fillcolor="#C8E6C9")
t.node("t_ffn", "Feed Forward (MLP)", fillcolor="#C8E6C9")
t.node("t_out", "Hidden States", fillcolor="#FFF9C4")

t.edges([
    ("t_in", "t_emb"),
    ("t_emb", "t_attn"),
    ("t_attn", "t_ffn"),
    ("t_ffn", "t_out"),
])

display(Markdown("## 1. Transformer (base comum)"))
display(t)

# ============================================================
# 2. BERT (ENCODER)
# ============================================================
b = Digraph("BERT", format="png")
b.attr(rankdir="LR", size="10,4")
b.attr("node", shape="rect", style="rounded,filled")

b.node("b_in", "Input Tokens")
b.node("b_emb", "Embedding", fillcolor="#BBDEFB")

with b.subgraph(name="cluster_enc") as c:
    c.attr(label="Encoder x12\nBidirectional Attention", style="rounded")
    c.node("b_attn", "Self-Attention\n(all tokens see all)", fillcolor="#C8E6C9")
    c.node("b_ffn", "Feed Forward", fillcolor="#C8E6C9")

b.node("b_out", "Contextual Representations\n(understanding)", fillcolor="#FFF9C4")

b.edges([
    ("b_in", "b_emb"),
    ("b_emb", "b_attn"),
    ("b_attn", "b_ffn"),
    ("b_ffn", "b_out"),
])

display(Markdown("## 2. BERT (Encoder bidirecional)"))
display(b)

# ============================================================
# 3. GPT-2 (DECODER)
# ============================================================
g = Digraph("GPT2", format="png")
g.attr(rankdir="LR", size="10,4")
g.attr("node", shape="rect", style="rounded,filled")

g.node("g_in", "Input Tokens")
g.node("g_emb", "Embedding", fillcolor="#BBDEFB")

with g.subgraph(name="cluster_dec") as c:
    c.attr(label="Decoder x12\nCausal Attention", style="rounded")
    c.node("g_attn", "Masked Self-Attention\n(past only)", fillcolor="#C8E6C9")
    c.node("g_ffn", "Feed Forward", fillcolor="#C8E6C9")

g.node("g_out", "Next Token Prediction\n(generation)", fillcolor="#FFF9C4")

g.edges([
    ("g_in", "g_emb"),
    ("g_emb", "g_attn"),
    ("g_attn", "g_ffn"),
    ("g_ffn", "g_out"),
])

display(Markdown("## 3. GPT-2 (Decoder causal)"))
display(g)

# ============================================================
# INSTALAÇÃO DAS BIBLIOTECAS (rodar uma vez no Colab)
# ============================================================
!pip install rich torchinfo

import torch
from rich import print
from rich.panel import Panel
from rich.syntax import Syntax
from torchinfo import summary
from IPython.display import display, Markdown

# ============================================================
# TÍTULO BONITO NO NOTEBOOK
# ============================================================
display(Markdown("# Arquitetura GPT-2 e BERT — Visão Geral"))

# ============================================================
# VISUALIZAÇÃO DO GPT-2
# ============================================================
print(Panel.fit(
    "ARQUITETURA GPT-2 — VISÃO GERAL",
    border_style="cyan"
))

syntax = Syntax(str(gpt_model), "python", theme="monokai", line_numbers=False)
print(syntax)

# ============================================================
# VISUALIZAÇÃO DO BERT
# ============================================================
print(Panel.fit(
    "BERT — ENCODER BIDIRECIONAL",
    border_style="green"
))

print(bert_model.config)

# ============================================================
# TORCHINFO (FORMA CORRETA PARA TRANSFORMERS)
# ============================================================
dummy_input = torch.zeros(
    (1, 128),
    dtype=torch.long,
    device=device
)

summary(
    bert_model,
    input_data={"input_ids": dummy_input},
    device=str(device)
)

# ============================================================
# (OPCIONAL) RESUMO DO GPT-2 TAMBÉM
# ============================================================
summary(
    gpt_model,
    input_data={"input_ids": dummy_input},
    device=str(device)
)

# ============================================================
# PARTE C — INFERÊNCIA (GERAÇÃO) COM MODELO CAUSAL (GPT-2)
# Controle: temperatura, top-p e KV cache
# ============================================================

# -----------------------------
# CÉLULA 25 — Carregar modelo causal (GPT-2)
# -----------------------------
gpt_name = "gpt2"                                              # Modelo causal leve e didático
gpt_tokenizer = AutoTokenizer.from_pretrained(gpt_name)        # Tokenizer BPE do GPT-2
gpt_model = AutoModelForCausalLM.from_pretrained(gpt_name)     # Modelo causal para geração
gpt_model.to(device)                                           # Move para GPU/CPU
gpt_model.eval()                                               # Modo avaliação

print("\n[GPT] Modelo carregado:", gpt_name)

print("\n==============================")
print("[ARQUITETURA GPT-2 — VISÃO GERAL]")
print("==============================")
print(gpt_model)

# -----------------------------
# CÉLULA 26 — Função de geração controlada (temperatura, top-p, cache)
# -----------------------------
def generate_text(prompt, max_new_tokens=80, temperature=1.0, top_p=1.0, use_cache=True):
    inputs = gpt_tokenizer(prompt, return_tensors="pt").to(device)  # Tokeniza prompt -> device

    with torch.no_grad():                                       # Inferência sem gradiente
        output = gpt_model.generate(
            **inputs,                                           # input_ids e attention_mask
            max_new_tokens=max_new_tokens,                       # Quantos tokens novos gerar
            do_sample=True,                                      # Ativa amostragem (não greedy)
            temperature=temperature,                             # Temperatura (controle de aleatoriedade)
            top_p=top_p,                                         # Top-p (nucleus sampling)
            use_cache=use_cache,                                 # KV cache (speedup)
            pad_token_id=gpt_tokenizer.eos_token_id              # Evita warnings de padding
        )

    return gpt_tokenizer.decode(output[0], skip_special_tokens=True) # Decodifica tokens -> texto

# -----------------------------
# CÉLULA 27 — Teste base de geração
# -----------------------------
prompt = "Transformers are models that"
print("\n[PROMPT]")
print(prompt)

print("\n[GERAÇÃO BASE]")
print(generate_text(prompt, temperature=0.7, top_p=0.9))

# -----------------------------
# CÉLULA 28 — Experimento: temperatura
# -----------------------------
for temp in [0.2, 0.7, 1.0]:
    print("\n==============================")
    print(f"[TEMPERATURA = {temp}]")
    print("==============================")
    print(generate_text(prompt, temperature=temp, top_p=0.9))

# -----------------------------
# CÉLULA 29 — Experimento: top-p
# -----------------------------
for p in [0.3, 0.6, 0.9]:
    print("\n==============================")
    print(f"[TOP-P = {p}]")
    print("==============================")
    print(generate_text(prompt, temperature=0.7, top_p=p))

# -----------------------------
# CÉLULA 30 — KV cache: comparação de tempo (com e sem cache)
# -----------------------------
def timed_generation(use_cache):
    start = time.time()                                        # Início do cronômetro
    _ = generate_text(
        "Large language models rely on attention mechanisms",
        max_new_tokens=120,
        temperature=0.7,
        top_p=0.9,
        use_cache=use_cache
    )
    return time.time() - start                                  # Tempo total

t_cache = timed_generation(True)                                # Tempo com cache
t_no_cache = timed_generation(False)                            # Tempo sem cache

print("\n[KV CACHE — TEMPO]")
print(f"Com cache : {t_cache:.4f} s")
print(f"Sem cache : {t_no_cache:.4f} s")

# ============================================================
# PARTE D — PRÁTICA 1: INFERÊNCIA INTERATIVA (PROMPTS + VARIAÇÃO)
# ============================================================

# -----------------------------
# CÉLULA 31 — Prompt direto vs "step by step" (demonstração didática)
# -----------------------------
prompt_direct = "What is 27 + 58?"
prompt_step = "Solve step by step: 27 + 58."

print("\n=== Direto ===")
print(generate_text(prompt_direct, temperature=0.7, top_p=0.9, max_new_tokens=40))

print("\n=== Step by step ===")
print(generate_text(prompt_step, temperature=0.7, top_p=0.9, max_new_tokens=80))

# -----------------------------
# CÉLULA 32 — Variabilidade com temperatura mais alta
# -----------------------------
for i in range(3):
    print(f"\n--- Amostra {i+1} (temp=0.9) ---")
    print(generate_text(prompt_step, temperature=0.9, top_p=0.95, max_new_tokens=80))

"""#Prática 1 – Inferência Interativa: experimentação guiada com prompts e parâmetros de geração, explorando variação de temperatura, top-p e raciocínio via chain-of-thought.

"""

# ============================================================
# NOTEBOOK-GABARITO — PRÁTICA 1: INFERÊNCIA INTERATIVA
# Mini-curso: Large Language Models e Agentes (MC-CD05)
# Uso exclusivo do professor / monitor
# ============================================================

# ============================================================
# CÉLULA 0 — SETUP
# ============================================================
!pip -q install transformers torch numpy

import torch
import time
import numpy as np
from transformers import AutoTokenizer, AutoModelForCausalLM

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)

# ============================================================
# CÉLULA 1 — CARREGAR MODELO CAUSAL (GPT-2)
# ============================================================
gpt_name = "gpt2"
tokenizer = AutoTokenizer.from_pretrained(gpt_name)
model = AutoModelForCausalLM.from_pretrained(gpt_name)
model.to(device)
model.eval()

print("\n[OK] Modelo carregado:", gpt_name)
print(model)

# ============================================================
# CÉLULA 2 — FUNÇÃO DE GERAÇÃO CONTROLADA
# ============================================================
def generate_text(
    prompt,
    temperature=0.7,
    top_p=0.9,
    max_new_tokens=80,
    use_cache=True
):
    inputs = tokenizer(prompt, return_tensors="pt").to(device)

    with torch.no_grad():
        out = model.generate(
            **inputs,
            do_sample=True,
            temperature=temperature,
            top_p=top_p,
            max_new_tokens=max_new_tokens,
            use_cache=use_cache,
            pad_token_id=tokenizer.eos_token_id
        )

    return tokenizer.decode(out[0], skip_special_tokens=True)

# ============================================================
# PARTE A — TEMPERATURA
# ============================================================
print("\n" + "="*70)
print("PARTE A — VARIAÇÃO DE TEMPERATURA")
print("="*70)

prompt = "Transformers are models that"
temps = [0.2, 0.7, 1.0]
outputs_temp = {}

for t in temps:
    text = generate_text(prompt, temperature=t, top_p=0.9)
    outputs_temp[t] = text
    print("\n------------------------------")
    print(f"TEMPERATURA = {t}")
    print("------------------------------")
    print(text)

print("""
RESPOSTA ESPERADA (PROFESSOR):

- τ = 0.2:
  Texto mais rígido, previsível e com padrões dominantes.
  Forte concentração da distribuição softmax.

- τ = 0.7:
  Melhor equilíbrio entre coerência e diversidade.
  Continuação plausível sem colapso.

- τ = 1.0:
  Maior diversidade e deriva semântica.
  Distribuição mais achatada.

CONCLUSÃO:
Temperatura controla a concentração da distribuição:
τ baixo amplifica logits dominantes;
τ alto espalha a massa de probabilidade.
""")

# ============================================================
# PARTE B — TOP-P (NUCLEUS SAMPLING)
# ============================================================
print("\n" + "="*70)
print("PARTE B — VARIAÇÃO DE TOP-P")
print("="*70)

ps = [0.3, 0.6, 0.9]
outputs_p = {}

for p in ps:
    text = generate_text(prompt, temperature=0.7, top_p=p)
    outputs_p[p] = text
    print("\n------------------------------")
    print(f"TOP-P = {p}")
    print("------------------------------")
    print(text)

print("""
RESPOSTA ESPERADA (PROFESSOR):

- top-p = 0.3:
  Suporte amostral muito pequeno.
  Repetição intensa e loops de alta probabilidade.

- top-p = 0.6:
  Diversidade moderada com controle.

- top-p = 0.9:
  Maior variedade lexical e temática.

CONCLUSÃO:
Top-p controla o tamanho efetivo do conjunto de candidatos.
Valores baixos favorecem repetição;
valores altos favorecem exploração.
""")

# ============================================================
# PARTE C — CHAIN-OF-THOUGHT
# ============================================================
print("\n" + "="*70)
print("PARTE C — PROMPT DIRETO VS STEP BY STEP")
print("="*70)

prompt_direct = "What is 27 + 58?"
prompt_step = "Solve step by step: 27 + 58."

out_direct = generate_text(prompt_direct, temperature=0.7, top_p=0.9, max_new_tokens=40)
out_step   = generate_text(prompt_step, temperature=0.7, top_p=0.9, max_new_tokens=80)

print("\n=== PROMPT DIRETO ===")
print(out_direct)

print("\n=== PROMPT STEP BY STEP ===")
print(out_step)

print("""
RESPOSTA ESPERADA (PROFESSOR):

- O modelo não garante correção aritmética.
- "Step by step" induz estilo explicativo, não capacidade lógica.
- A sequência pode conter erros, misturas ou deriva.

CONCLUSÃO:
Chain-of-thought é heurística de geração textual,
não um mecanismo formal de raciocínio matemático.
""")

# ============================================================
# PARTE D — DIAGNÓSTICO DE PARÂMETROS
# ============================================================
print("\n" + "="*70)
print("PARTE D — DIAGNÓSTICO")
print("="*70)

print("""
PROBLEMA:
Texto repetitivo e preso em estruturas fixas.

AJUSTE PRIORITÁRIO:
Aumentar top-p.

JUSTIFICATIVA:
O problema é suporte amostral pequeno.
Top-p atua diretamente no conjunto de tokens candidatos.
Temperatura pode ser ajustada depois.
""")

# ============================================================
# PARTE E — KV CACHE (CUSTO COMPUTACIONAL)
# ============================================================
print("\n" + "="*70)
print("PARTE E — KV CACHE")
print("="*70)

def timed(use_cache):
    start = time.time()
    _ = generate_text(
        "Large language models rely on attention mechanisms",
        temperature=0.7,
        top_p=0.9,
        max_new_tokens=120,
        use_cache=use_cache
    )
    return time.time() - start

t_cache = timed(True)
t_no_cache = timed(False)

print(f"Com cache : {t_cache:.4f} s")
print(f"Sem cache : {t_no_cache:.4f} s")

print("""
LEITURA CORRETA:
KV cache não altera o texto gerado.
Ele reduz custo computacional evitando recomputar o prefixo.
Diferença pequena aqui por ser GPT-2 e sequência curta.
Em modelos grandes, a economia é significativa.
""")

# ============================================================
# FECHAMENTO
# ============================================================
print("\n" + "="*70)
print("CHECKLIST FINAL DO ALUNO")
print("="*70)

print("""
1. Temperatura controla concentração da distribuição.
2. Top-p controla o conjunto efetivo de candidatos.
3. Prompt altera o contexto, não os pesos do modelo.
4. Chain-of-thought é estilo, não garantia de correção.
5. KV cache altera latência, não semântica.

FIM DO NOTEBOOK-GABARITO.
""")

"""#Prompt engineering e in-context learning: few-shot, self-consistency e limites de contexto.

"""

import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# Define o device PRIMEIRO
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)

# Modelo instruído leve
t5_name = "google/flan-t5-small"
t5_tokenizer = AutoTokenizer.from_pretrained(t5_name)
t5_model = AutoModelForSeq2SeqLM.from_pretrained(t5_name).to(device)
t5_model.eval()

print("[OK] Modelo instruído carregado:", t5_name)

# ============================================================
# CÉLULA 7 — FUNÇÃO DE GERAÇÃO PARA O MODELO INSTRUÍDO (FLAN-T5)
# ============================================================
def generate_t5(
    prompt,
    temperature=0.7,
    top_p=0.9,
    max_new_tokens=64,
    num_return_sequences=1,
    do_sample=True
):
    inputs = t5_tokenizer(prompt, return_tensors="pt", truncation=True).to(device)

    with torch.no_grad():
        out = t5_model.generate(
            **inputs,
            do_sample=do_sample,
            temperature=temperature,
            top_p=top_p,
            max_new_tokens=max_new_tokens,
            num_return_sequences=num_return_sequences
        )

    texts = [t5_tokenizer.decode(x, skip_special_tokens=True) for x in out]
    return texts if num_return_sequences > 1 else texts[0]

# ============================================================
# PARTE F — FEW-SHOT / IN-CONTEXT LEARNING (ICL)
# Objetivo didático:
# - Mostrar que "exemplos no prompt" mudam o comportamento,
#   mesmo sem alterar pesos.
# - Mostrar que o formato (template) é quase tudo.
# ============================================================
print("\n" + "="*70)
print("PARTE F — FEW-SHOT / IN-CONTEXT LEARNING (ICL)")
print("="*70)

task = "Classifique o sentimento como POSITIVO ou NEGATIVO."

zero_shot = f"""{task}

Texto: "Este filme foi um desperdício de tempo."
Sentimento:"""

few_shot = f"""{task}

Texto: "Adorei, foi excelente e emocionante."
Sentimento: POSITIVO

Texto: "Péssimo. Não recomendo para ninguém."
Sentimento: NEGATIVO

Texto: "Este filme foi um desperdício de tempo."
Sentimento:"""

print("\n--- ZERO-SHOT (sem exemplos) ---")
print(generate_t5(zero_shot, temperature=0.0, do_sample=False))

print("\n--- FEW-SHOT (com exemplos) ---")
print(generate_t5(few_shot, temperature=0.0, do_sample=False))

print("""
RESPOSTA ESPERADA (PROFESSOR):

- Zero-shot:
  Pode acertar, mas é menos estável e mais sensível à redação.
- Few-shot:
  O modelo aprende o "formato da tarefa" pelo contexto:
  (Texto -> rótulo) e reproduz o padrão no exemplo novo.

PONTO-CHAVE:
ICL não muda pesos; muda o comportamento por condicionamento do prompt.
O que "ensina" é o template + exemplos (demonstrações).
""")

# ============================================================
# PARTE F2 — "O TEMPLATE É O MODELO": mesma tarefa, template ruim vs bom
# ============================================================
print("\n" + "="*70)
print("PARTE F2 — TEMPLATE RUIM vs TEMPLATE BOM")
print("="*70)

template_ruim = f"""{task}
"Este filme foi um desperdício de tempo." ->"""

template_bom = f"""{task}

Responda APENAS com: POSITIVO ou NEGATIVO.

Texto: "Este filme foi um desperdício de tempo."
Sentimento:"""

print("\n--- TEMPLATE RUIM ---")
print(generate_t5(template_ruim, temperature=0.0, do_sample=False))

print("\n--- TEMPLATE BOM ---")
print(generate_t5(template_bom, temperature=0.0, do_sample=False))

print("""
RESPOSTA ESPERADA (PROFESSOR):

- Template ruim:
  Saída pode vir prolixa, ambígua, ou com rótulo fora do esperado.
- Template bom:
  Restringe o espaço de saída (output space) e reduz ambiguidade.

PONTO-CHAVE:
Prompt engineering = projetar entradas para controlar o *formato* e o *espaço de resposta*.
""")

# ============================================================
# PARTE G — SELF-CONSISTENCY (amostragem múltipla + votação)
# Objetivo didático:
# - Mostrar que amostrar várias vezes e agregar melhora robustez.
# - Não é "mágica": é reduzir variância via ensemble.
# ============================================================
print("\n" + "="*70)
print("PARTE G — SELF-CONSISTENCY (MULTI-AMOSTRAGEM + VOTO)")
print("="*70)

from collections import Counter
import re

def extract_label(text):
    # Tenta capturar POSITIVO/NEGATIVO de forma robusta
    t = text.strip().upper()
    m = re.search(r"\b(POSITIVO|NEGATIVO)\b", t)
    return m.group(1) if m else None

prompt_sc = """Classifique o sentimento como POSITIVO ou NEGATIVO.
Responda APENAS com POSITIVO ou NEGATIVO.

Texto: "O atendimento foi ótimo, mas o produto quebrou no dia seguinte."
Sentimento:"""

samples = generate_t5(
    prompt_sc,
    temperature=0.9,
    top_p=0.9,
    max_new_tokens=8,
    num_return_sequences=15,
    do_sample=True
)

labels = [extract_label(s) for s in samples]
labels_clean = [l for l in labels if l is not None]

print("\nAmostras brutas:")
for i, s in enumerate(samples, 1):
    print(f"{i:02d}. {s}")

counts = Counter(labels_clean)
print("\nContagem de rótulos:", dict(counts))

if len(counts) > 0:
    voted = counts.most_common(1)[0][0]
else:
    voted = "INDETERMINADO"

print("\nVOTO FINAL (self-consistency):", voted)

print("""
RESPOSTA ESPERADA (PROFESSOR):

- As amostras variam (principalmente em caso ambíguo).
- A votação tende a estabilizar a decisão.
- Isso é "redução de variância" por ensemble: amostras ~ hipóteses.

PONTO-CHAVE:
Self-consistency melhora robustez quando há múltiplos caminhos plausíveis.
Mas não garante verdade — só aumenta consistência interna.
""")

# ============================================================
# PARTE G2 — SELF-CONSISTENCY EM RESPOSTA NUMÉRICA (com parsing)
# Exemplo: um problema simples, mas com ruído textual possível.
# ============================================================
print("\n" + "="*70)
print("PARTE G2 — SELF-CONSISTENCY EM RESPOSTA NUMÉRICA")
print("="*70)

def extract_int(text):
    m = re.search(r"(-?\d+)", text.strip())
    return int(m.group(1)) if m else None

prompt_math = """Responda APENAS com um número inteiro.
Pergunta: 27 + 58 = ?"""

samples = generate_t5(
    prompt_math,
    temperature=0.8,
    top_p=0.9,
    max_new_tokens=6,
    num_return_sequences=20,
    do_sample=True
)

vals = [extract_int(s) for s in samples]
vals_clean = [v for v in vals if v is not None]

print("\nAmostras:")
for i, s in enumerate(samples, 1):
    print(f"{i:02d}. {s}")

counts = Counter(vals_clean)
print("\nContagem:", dict(counts))

if len(counts) > 0:
    voted = counts.most_common(1)[0][0]
else:
    voted = None

print("\nVOTO FINAL:", voted)

print("""
RESPOSTA ESPERADA (PROFESSOR):

- Mesmo com instrução "apenas um inteiro", pode aparecer ruído.
- Parsing + votação transforma várias amostras em um "resultado agregado".
- Em problemas simples, costuma convergir no valor correto (85),
  mas o objetivo didático é o *método*.

PONTO-CHAVE:
Self-consistency + parsing = um padrão de engenharia:
(gerar várias hipóteses) -> (extrair estrutura) -> (agregar).
""")

# ============================================================
# PARTE H — LIMITES DE CONTEXTO (token budget, truncamento)
# Célula única, autocontida e pronta para rodar
# ============================================================

# Instala dependências (silencioso)
!pip -q install sentencepiece transformers

import torch
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    AutoModelForSeq2SeqLM
)

# ------------------------------------------------------------
# Device
# ------------------------------------------------------------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)

# ------------------------------------------------------------
# Carregar GPT-2 (modelo causal, NÃO instruction-tuned)
# ------------------------------------------------------------
gpt2_name = "gpt2"
tokenizer = AutoTokenizer.from_pretrained(gpt2_name)
gpt2_model = AutoModelForCausalLM.from_pretrained(gpt2_name).to(device)
gpt2_model.eval()

print("[OK] GPT-2 carregado")

# ------------------------------------------------------------
# Carregar FLAN-T5-small (instruction-tuned)
# ------------------------------------------------------------
t5_name = "google/flan-t5-small"
t5_tokenizer = AutoTokenizer.from_pretrained(t5_name)
t5_model = AutoModelForSeq2SeqLM.from_pretrained(t5_name).to(device)
t5_model.eval()

print("[OK] FLAN-T5-small carregado")

# ------------------------------------------------------------
# PARTE H — LIMITES DE CONTEXTO
# ------------------------------------------------------------
print("\n" + "="*70)
print("PARTE H — LIMITES DE CONTEXTO (TOKENS)")
print("="*70)

# Janela de contexto do GPT-2
ctx_gpt2 = (
    getattr(gpt2_model.config, "n_positions", None)
    or getattr(gpt2_model.config, "max_position_embeddings", None)
)

print("Janela (GPT-2) n_positions:", ctx_gpt2)

# ------------------------------------------------------------
# Funções utilitárias: contagem de tokens
# ------------------------------------------------------------
def count_tokens_gpt2(text):
    return len(tokenizer.encode(text))

def count_tokens_t5(text):
    return len(t5_tokenizer.encode(text))

# ------------------------------------------------------------
# Exemplo simples de contagem
# ------------------------------------------------------------
print("\nExemplo de contagem de tokens:")
demo = "Transformers are models that"
print("Texto:", demo)
print("GPT-2 tokens:", count_tokens_gpt2(demo))
print("T5 tokens  :", count_tokens_t5(demo))

print("""
LEITURA CORRETA:
- O modelo não "vê" caracteres: vê tokens.
- Limite real = limite de tokens de entrada
  (e às vezes entrada + saída).
""")

# ------------------------------------------------------------
# Demonstração de truncamento explícito
# ------------------------------------------------------------
print("\n--- Demonstração de truncamento ---")

long_text = "INICIO " + ("bla " * 1500) + "FIM"
tokens = tokenizer.encode(long_text)

print("Tokens totais:", len(tokens))
print("Janela GPT-2   :", ctx_gpt2)

truncated_tokens = tokens[:ctx_gpt2]

print("Tokens após truncamento:", len(truncated_tokens))

print("\nINÍCIO DO TEXTO TRUNCADO:")
print(tokenizer.decode(truncated_tokens[:40]))

print("\nFIM DO TEXTO TRUNCADO:")
print(tokenizer.decode(truncated_tokens[-40:]))

print("""
INTERPRETAÇÃO DIDÁTICA:
- Quando o texto excede a janela, o modelo NÃO vê tudo.
- Em setups comuns, o começo do contexto é perdido.
- Estratégias práticas:
  • keep the tail
  • janela deslizante (sliding window)
  • resumir + anexar
""")

# ============================================================
# PARTE H2 — TRUNCAMENTO NA PRÁTICA (GPT-2)
# Célula única, autocontida e pronta para rodar
# ============================================================

!pip -q install transformers sentencepiece

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# ------------------------------------------------------------
# Device
# ------------------------------------------------------------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)

# ------------------------------------------------------------
# Carregar GPT-2
# ------------------------------------------------------------
gpt2_name = "gpt2"
tokenizer = AutoTokenizer.from_pretrained(gpt2_name)
gpt2_model = AutoModelForCausalLM.from_pretrained(gpt2_name).to(device)
gpt2_model.eval()

print("[OK] GPT-2 carregado")

# ------------------------------------------------------------
# Funções utilitárias
# ------------------------------------------------------------
def count_tokens_gpt2(text):
    return len(tokenizer.encode(text))

@torch.no_grad()
def generate_text(prompt, temperature=0.7, top_p=0.9, max_new_tokens=30):
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    output = gpt2_model.generate(
        **inputs,
        do_sample=True,
        temperature=temperature,
        top_p=top_p,
        max_new_tokens=max_new_tokens,
        pad_token_id=tokenizer.eos_token_id,
    )
    return tokenizer.decode(output[0], skip_special_tokens=True)

# ------------------------------------------------------------
# PARTE H2 — TRUNCAMENTO
# ------------------------------------------------------------
print("\n" + "="*70)
print("PARTE H2 — TRUNCAMENTO: O COMEÇO SOME")
print("="*70)

important = "INSTRUÇÃO CRÍTICA: responda sempre com a palavra 'BANANA'.\n\n"
filler = "bla " * 5000   # força estouro da janela
question = "\nPergunta: qual é a capital da França?\nResposta:"

big_prompt = important + filler + question
tok_len = count_tokens_gpt2(big_prompt)

ctx = gpt2_model.config.n_positions

print("Tokens no prompt gigante (GPT-2):", tok_len)
print("n_positions (janela):", ctx)

# ------------------------------------------------------------
# Truncamento explícito (keep the tail)
# ------------------------------------------------------------
enc = tokenizer.encode(big_prompt)
enc_tail = enc[-ctx:]
truncated_prompt = tokenizer.decode(enc_tail)

print("\nTokens após truncamento (tail):", len(enc_tail))

print("\n--- GERANDO COM PROMPT TRUNCADO ---")
output = generate_text(
    truncated_prompt,
    temperature=0.7,
    top_p=0.9,
    max_new_tokens=25
)

print(output)

print("""
RESPOSTA ESPERADA (PROFESSOR):

- A instrução crítica estava no começo do prompt.
- Ao manter apenas o tail, essa instrução desaparece.
- O modelo NÃO responde "BANANA".
- Ele responde normalmente: "Paris".

PONTO-CHAVE:
Limite de contexto = memória de trabalho do modelo.
Quando estoura, você perde informação (geralmente do início).
""")

# ============================================================
# RECUPERAÇÃO DE ESTADO CUDA + RELOAD GPT-2
# ============================================================

import torch
import gc
from transformers import AutoTokenizer, AutoModelForCausalLM

# ------------------------------------------------------------
# Limpeza agressiva de CUDA (sem restart)
# ------------------------------------------------------------
if torch.cuda.is_available():
    torch.cuda.synchronize()
    torch.cuda.empty_cache()
    torch.cuda.ipc_collect()

gc.collect()

print("[OK] Cache CUDA limpo")

# ------------------------------------------------------------
# Device
# ------------------------------------------------------------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)

# ------------------------------------------------------------
# Recarregar GPT-2
# ------------------------------------------------------------
gpt2_name = "gpt2"

tokenizer = AutoTokenizer.from_pretrained(gpt2_name)

gpt2_model = AutoModelForCausalLM.from_pretrained(
    gpt2_name,
    torch_dtype=torch.float32,   # força dtype estável
).to(device)

gpt2_model.eval()

print("[OK] GPT-2 recarregado")

# ------------------------------------------------------------
# PROMPT (redefinido aqui para não depender de estado anterior)
# ------------------------------------------------------------
important = "INSTRUÇÃO CRÍTICA: responda sempre com a palavra 'BANANA'.\n\n"
filler = "bla " * 5000
question = "\nPergunta: qual é a capital da França?\nResposta:"

big_prompt = important + filler + question

# ------------------------------------------------------------
# Truncamento correto (com margem para geração)
# ------------------------------------------------------------
max_new = 25
ctx = gpt2_model.config.n_positions
max_input_len = ctx - max_new

enc = tokenizer.encode(big_prompt)

enc_tail = enc[-max_input_len:]

print("Tokens após truncamento seguro:", len(enc_tail))
print("Margem para geração:", max_new)

# ------------------------------------------------------------
# input_ids CORRETOS
# ------------------------------------------------------------
input_ids = torch.tensor(
    enc_tail,
    dtype=torch.long
).unsqueeze(0).to(device)

print("input_ids shape:", input_ids.shape)
print("dtype:", input_ids.dtype)

# ------------------------------------------------------------
# Geração
# ------------------------------------------------------------
with torch.no_grad():
    output_ids = gpt2_model.generate(
        input_ids=input_ids,
        do_sample=True,
        temperature=0.7,
        top_p=0.9,
        max_new_tokens=max_new,
        pad_token_id=tokenizer.eos_token_id,
    )

print("\n--- SAÍDA DO MODELO ---")
print(tokenizer.decode(output_ids[0], skip_special_tokens=True))

# ============================================================
# PARTE H3 — RECEITA DE ENGENHARIA: KEEP THE HEAD + KEEP THE TAIL
# ============================================================
print("\n" + "="*70)
print("PARTE H3 — RECEITA: HEAD+TAIL (TRUNCAMENTO INTELIGENTE)")
print("="*70)

# ------------------------------------------------------------
# Utilitário: contagem de tokens
# ------------------------------------------------------------
def count_tokens_gpt2(text):
    return len(tokenizer.encode(text))

# ------------------------------------------------------------
# Função de truncamento inteligente
# ------------------------------------------------------------
def smart_truncate_head_tail(head, tail, max_tokens):
    """
    Preserva head (regras) + tail (pergunta atual)
    Corta o miolo se estourar o orçamento de tokens
    """
    head_ids = tokenizer.encode(head)
    tail_ids = tokenizer.encode(tail)

    if len(head_ids) + len(tail_ids) > max_tokens:
        space_for_tail = max_tokens - len(head_ids)

        if space_for_tail <= 0:
            # Caso extremo: só cabe parte do head
            return tokenizer.decode(head_ids[:max_tokens])

        tail_ids = tail_ids[-space_for_tail:]

    return tokenizer.decode(head_ids + tail_ids)

# ------------------------------------------------------------
# Prompt de teste
# ------------------------------------------------------------
head = (
    "REGRA: responda com UMA ÚNICA palavra. "
    "Se não souber, responda 'N/A'.\n\n"
)

tail = filler + "\nPergunta: diga a palavra BANANA.\nResposta:"

ctx = gpt2_model.config.n_positions

smart_prompt = smart_truncate_head_tail(head, tail, ctx)

print("Tokens (smart prompt):", count_tokens_gpt2(smart_prompt))

# ------------------------------------------------------------
# Geração (com margem segura)
# ------------------------------------------------------------
max_new = 10
enc = tokenizer.encode(smart_prompt)

# garante espaço para geração
enc = enc[: ctx - max_new]

input_ids = torch.tensor(
    enc,
    dtype=torch.long
).unsqueeze(0).to(device)

with torch.no_grad():
    output_ids = gpt2_model.generate(
        input_ids=input_ids,
        do_sample=True,
        temperature=0.7,
        top_p=0.9,
        max_new_tokens=max_new,
        pad_token_id=tokenizer.eos_token_id,
    )

print("\nSaída:")
print(tokenizer.decode(output_ids[0], skip_special_tokens=True))

print("""
RESPOSTA ESPERADA (PROFESSOR):

- O HEAD preserva regras de formato e comportamento.
- O TAIL preserva a pergunta atual.
- O "miolo" (histórico antigo / filler) é descartado.

PONTO-CHAVE:
Gerenciamento de contexto é parte central da engenharia
de RAG, agentes e sistemas conversacionais.
""")

# ============================================================
# PARTE I — FECHAMENTO (PROMPT ENGINEERING + ICL + CONTEXTO)
# ============================================================
print("\n" + "="*70)
print("CHECKLIST FINAL — PROMPT ENGINEERING & ICL")
print("="*70)

print("""
1) Few-shot (ICL) funciona por condicionamento no prompt:
   exemplos + template => comportamento.

2) Template importa: restringir o espaço de saída reduz ambiguidade.

3) Self-consistency = amostrar várias hipóteses e agregar (voto / média / parsing).
   É redução de variância, não garantia de verdade.

4) Limite de contexto = limite de tokens.
   Se estourar, você perde informação (geralmente do início).

5) Receita prática: preservar HEAD (regras) + TAIL (pergunta atual) e cortar o miolo.

FIM — EXTENSÃO DO NOTEBOOK-GABARITO.
""")