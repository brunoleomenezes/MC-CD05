# -*- coding: utf-8 -*-
"""03_LNCC_bruno_MC-CD05_100226.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12UVOA6ouswjddd0SvKM4x7G4xxVeYfeV

##**Large Language Models - MC-CD05**
*Professor: Bruno Menezes e Daniel Senna (LNCC)*
"""

!pip install -q transformers accelerate bitsandbytes peft torch

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig

# Define o identificador do modelo no Hugging Face Hub (org/nome-do-modelo).
# Aqui escolhemos o Qwen2 1.5B já ajustado para instruções (“Instruct”),
# ou seja, tende a seguir prompts no estilo chat/pergunta-resposta.
model_name = "Qwen/Qwen2-1.5B-Instruct"

# Carrega o tokenizador correspondente a esse modelo.
# O tokenizador é responsável por transformar texto (string) em tokens (ids inteiros)
# que o modelo entende, e também por fazer o caminho inverso (ids -> texto).
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Carrega o modelo de linguagem causal (CausalLM), isto é, um modelo que prevê o próximo token
# dado um prefixo, base do mecanismo de geração autoregressiva.
model = AutoModelForCausalLM.from_pretrained(
    # Informa qual checkpoint exato baixar/carregar (mesmo nome usado no tokenizador).
    model_name,

    # Define o dtype (tipo numérico) dos pesos do modelo ao carregar.
    # float16 reduz o uso de memória (aprox. metade do float32) e costuma acelerar em GPU,
    # com pequena perda (ou nenhuma) em qualidade para inferência.
    torch_dtype=torch.float16,

    # Pede ao Transformers para decidir automaticamente onde colocar o modelo:
    # - se houver GPU, ele tende a mover o modelo para CUDA;
    # - se houver várias GPUs, pode “shardar” (particionar) camadas entre elas;
    # - se não houver GPU suficiente, pode colocar parte na CPU (dependendo do ambiente).
    # Isso é útil em HPC e em máquinas com múltiplas GPUs, pois evita configuração manual.
    device_map="auto"
)

# Define o prompt (texto de entrada) que será passado ao modelo.
# Aqui pedimos uma explicação sobre por que mecanismos de atenção são importantes em Transformers.
prompt = "Explain why attention mechanisms are important in transformers."

# Tokeniza o prompt: converte a string em tensores PyTorch (ids de tokens, máscaras, etc.).
# return_tensors="pt" => retorna tensores no formato do PyTorch.
# .to(model.device) move esses tensores para o mesmo dispositivo do modelo (GPU/CPU),
# evitando erro de “device mismatch” e garantindo inferência eficiente.
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

# Define uma função auxiliar para gerar texto, variando temperatura e top-p.
# Isso permite comparar comportamentos do modelo sob diferentes hiperparâmetros de amostragem.
def generate(temp, top_p):
    # Chama o método de geração autoregressiva do modelo.
    outputs = model.generate(
        # Desempacota o dicionário 'inputs' (por exemplo input_ids, attention_mask)
        # como argumentos nomeados para model.generate.
        **inputs,

        # Define o máximo de tokens novos gerados (não conta os tokens do prompt).
        # Controla custo, tempo e tamanho da resposta.
        max_new_tokens=120,

        # Controla a aleatoriedade: valores menores tornam a saída mais determinística,
        # valores maiores aumentam diversidade e risco de deriva/erro.
        temperature=temp,

        # Nucleus sampling: amostra apenas do menor conjunto de tokens cuja probabilidade
        # acumulada atinge 'top_p'. Tipicamente 0.8–0.95.
        top_p=top_p,

        # Habilita amostragem (sampling). Sem isso, o modelo tende a usar greedy/beam,
        # e temperatura/top_p podem não ter efeito como esperado.
        do_sample=True
    )

    # Decodifica os tokens gerados (ids) de volta para texto.
    # outputs[0] pega a primeira sequência do batch (aqui batch=1).
    # skip_special_tokens=True remove tokens especiais como <bos>, <eos>, etc.
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# Gera uma resposta com baixa temperatura (mais estável/determinística) e top_p=0.9,
# e imprime o resultado para comparação.
print("Low temperature:\n", generate(0.2, 0.9))

# Gera uma resposta com alta temperatura (mais diversa/arriscada) e top_p=0.9,
# e imprime o resultado para comparação.
print("\nHigh temperature:\n", generate(0.9, 0.9))

# Cria um objeto GenerationConfig, que é um contêiner de configurações padrão
# para controlar como o modelo irá gerar texto (decoding/inferência).
GenerationConfig(
    # Define a temperatura da amostragem.
    # Valores menores (ex.: 0.2–0.5) tornam a saída mais determinística/conservadora;
    # valores maiores (ex.: 0.8–1.2) aumentam diversidade e variabilidade.
    temperature=0.7,

    # Define o nucleus sampling (top-p): o modelo só amostra entre os tokens mais prováveis
    # até que a probabilidade acumulada desses tokens alcance 0.9.
    # Ajuda a evitar tokens muito improváveis, mantendo diversidade controlada.
    top_p=0.9,

    # Define o top-k: restringe a amostragem aos 50 tokens mais prováveis em cada passo.
    # Funciona como um corte “duro” por quantidade, complementando o corte “suave” do top-p.
    top_k=50
)

# Cria e armazena em 'gen_cfg' um objeto GenerationConfig, que centraliza
# os hiperparâmetros de geração (decoding) usados em chamadas model.generate().
gen_cfg = GenerationConfig(
    # Define a temperatura da amostragem.
    # Reduz (ex.: 0.2) -> saída mais determinística; aumenta (ex.: 0.9) -> mais diversidade.
    temperature=0.7,

    # Define o nucleus sampling (top-p): em cada passo, o modelo considera apenas
    # o menor conjunto de tokens mais prováveis cuja probabilidade acumulada >= 0.9.
    # Isso evita tokens muito improváveis, mantendo diversidade controlada.
    top_p=0.9,

    # Define top-k: em cada passo, restringe a escolha aos 50 tokens mais prováveis.
    # É um corte “duro” por quantidade, que pode complementar o top-p.
    top_k=50,

    # Ativa amostragem estocástica (sampling).
    # Sem do_sample=True, o modelo tende a usar greedy decoding, e temperature/top-p/top-k
    # podem ser ignorados (ou não produzir efeito prático).
    do_sample=True
)

# Gera uma continuação para o prompt já tokenizado em 'inputs' usando o modelo carregado.
# O resultado (sequência de ids gerados) será armazenado na variável 'outputs'.
outputs = model.generate(
    # Desempacota o dicionário 'inputs' (tipicamente contém 'input_ids' e 'attention_mask')
    # e passa esses tensores como argumentos nomeados para o método generate().
    **inputs,

    # Controla o tamanho da continuação: gera no máximo 50 tokens NOVOS,
    # sem contar os tokens do prompt original.
    max_new_tokens=50,

    # Habilita o KV cache (cache de chaves e valores da atenção) durante a geração.
    # Isso evita recomputar atenção para todo o prefixo a cada novo token,
    # reduzindo latência e custo computacional na inferência autoregressiva.
    use_cache=True
)

# Define um prompt com instrução explícita para "resolver passo a passo" (chain-of-thought).
# As aspas triplas permitem um texto multilinha, preservando quebras de linha.
cot_prompt = """
Solve step by step:

If a transformer has 12 layers and each layer has 12 heads,
how many attention heads are there in total?
"""

# Tokeniza o prompt: converte o texto em tensores PyTorch (por exemplo, input_ids e attention_mask).
# return_tensors="pt" retorna tensores no formato do PyTorch.
# .to(model.device) move os tensores para o mesmo dispositivo do modelo (GPU/CPU),
# evitando erro de dispositivo e melhorando a eficiência.
inputs = tokenizer(cot_prompt, return_tensors="pt").to(model.device)

# Gera uma resposta autoregressiva usando o modelo.
outputs = model.generate(
    # Desempacota o dicionário de entrada (input_ids, attention_mask, etc.) na chamada.
    **inputs,

    # Limita o tamanho da continuação: no máximo 150 tokens novos serão gerados
    # (sem contar os tokens do prompt).
    max_new_tokens=150,

    # Define a temperatura da amostragem (aqui moderada).
    # Em geral, temperatura mais baixa torna o passo a passo mais estável; mais alta aumenta variação.
    # Observação: para a temperatura realmente ter efeito, é comum usar do_sample=True;
    # dependendo do default do modelo, temperature pode ser ignorado em modo determinístico.
    temperature=0.7
)

# Decodifica a sequência gerada (ids) de volta para texto.
# outputs[0] pega a primeira saída do batch.
# skip_special_tokens=True remove tokens especiais (BOS/EOS, etc.) da string final.
print(tokenizer.decode(outputs[0], skip_special_tokens=True))

# Define um prompt few-shot (com exemplos) no formato Pergunta/Resposta.
# A ideia é dar ao modelo um "padrão" de como responder: primeiro mostramos um par Q/A completo
# e depois fazemos uma nova pergunta para ele continuar no mesmo estilo.
few_shot = """
Q: What is attention?
A: A mechanism that weights token interactions.

Q: What is a transformer?
A:
"""

# Tokeniza o prompt few-shot: converte o texto em tensores PyTorch (input_ids, attention_mask, etc.).
# return_tensors="pt" retorna tensores no formato do PyTorch.
# .to(model.device) move esses tensores para o mesmo dispositivo do modelo (GPU/CPU).
inputs = tokenizer(few_shot, return_tensors="pt").to(model.device)

# Gera a continuação do texto, seguindo o padrão induzido pelo few-shot.
outputs = model.generate(
    # Desempacota os tensores de entrada (por exemplo, input_ids e attention_mask) na chamada.
    **inputs,

    # Limita o tamanho da resposta: gera no máximo 80 tokens novos além do prompt.
    max_new_tokens=80,

    # Define uma temperatura baixa/moderada para reduzir aleatoriedade,
    # aumentando a chance de uma resposta mais direta e consistente com o exemplo dado.
    # Observação: para a temperatura ter efeito de amostragem, é comum usar do_sample=True;
    # dependendo do regime de geração ativo, a temperatura pode ter efeito limitado.
    temperature=0.3
)

# Converte a sequência gerada (ids) de volta para texto e imprime.
# outputs[0] pega a primeira saída do batch; skip_special_tokens=True remove tokens especiais.
print(tokenizer.decode(outputs[0], skip_special_tokens=True))

# Uma breve explicação
print("INT4 / FP8 = menor memória, mais throughput, leve perda de precisão")

# Importa do pacote PEFT (Parameter-Efficient Fine-Tuning) as classes/funções necessárias:
# - LoraConfig: objeto que descreve como aplicar LoRA (hiperparâmetros e onde inserir).
# - get_peft_model: função que "envolve" o modelo base e injeta os adapters LoRA nas camadas alvo.
from peft import LoraConfig, get_peft_model

# Cria a configuração do LoRA, definindo como os adapters serão adicionados ao modelo.
lora_config = LoraConfig(
    # r é o "rank" (posto) da decomposição de baixo-rank usada pelo LoRA.
    # r menor => menos parâmetros treináveis e menor custo; r maior => mais capacidade de adaptação.
    r=8,

    # lora_alpha é um fator de escala aplicado à atualização LoRA.
    # Em muitos setups, o ganho efetivo é proporcional a (lora_alpha / r), controlando a intensidade do adapter.
    lora_alpha=32,

    # Define em quais submódulos/camadas os adapters LoRA serão inseridos.
    # Aqui escolhemos projeções de atenção:
    # - q_proj: projeção que gera as Queries
    # - v_proj: projeção que gera os Values
    # Isso é comum porque Q/V influenciam diretamente o padrão de atenção e a composição contextual.
    target_modules=["q_proj", "v_proj"],

    # Dropout aplicado dentro do caminho LoRA durante o treinamento.
    # Ajuda a regularizar e reduzir overfitting, especialmente com poucos dados.
    lora_dropout=0.05,

    # Define como tratar o termo de bias (viés) nas camadas adaptadas:
    # "none" => não treina biases (mantém o viés do modelo base congelado),
    # o que reduz ainda mais o número de parâmetros treináveis.
    bias="none"
)

# Aplica a configuração LoRA ao modelo base, retornando um novo modelo "wrapper"
# com os adapters LoRA injetados nas camadas especificadas em target_modules.
lora_model = get_peft_model(model, lora_config)

# Imprime quantos parâmetros do modelo agora são treináveis versus o total.
# Serve para evidenciar o ganho do LoRA: normalmente apenas uma fração pequena (<< 1%) fica treinável.
lora_model.print_trainable_parameters()

# Percorre todos os parâmetros (pesos) do modelo com LoRA já aplicado.
# named_parameters() retorna pares (nome_do_parametro, tensor_do_parametro),
# onde o nome permite identificar de que parte do modelo aquele peso vem.
for name, param in lora_model.named_parameters():
    # Verifica se o nome do parâmetro contém a substring "lora".
    # Em modelos PEFT/LoRA, os adapters adicionados costumam ter "lora" no nome,
    # então esse teste filtra apenas os pesos novos inseridos pelo LoRA.
    if "lora" in name:
        # Imprime o nome do parâmetro LoRA encontrado e o seu formato (shape).
        # O shape é útil para entender dimensões e confirmar onde o LoRA foi injetado
        # (por exemplo, em projeções q_proj/v_proj) e qual o rank efetivo do adapter.
        print(name, param.shape)
        # Interrompe o loop após encontrar e imprimir o primeiro parâmetro LoRA.
        # Isso evita despejar uma lista longa de parâmetros e mantém a saída curta para aula.
        break

"""RLHF: caro, humano no loop.

DPO: preferência direta, mais simples.

GRPO: grupos de respostas.

DeepSeek-R1: raciocínio emergente via RL.
"""

benchmarks = [
    "MMLU-Pro",
    "AIME-24/25",
    "LiveBench",
    "GPQA"
]
benchmarks

"""LLMs não são só modelos.

São sistemas: dados, inferência, alinhamento, avaliação.
"""